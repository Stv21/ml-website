<!DOCTYPE html>
<html>
<head>
    <title>Decision Tree (heart.csv)</title>
    <meta charset="UTF-8">
</head>
<body>
<pre>
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, classification_report
from google.colab import drive
from sklearn import tree
import matplotlib.pyplot as plt

# 1. Mount Google Drive and Load Data
drive.mount('/content/drive')
file_path = "/content/drive/My Drive/heart.csv" 
try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Error: File not found at {file_path}")
    exit()
print("Data loaded successfully.")

# 2. Prepare Data
df.dropna(inplace=True)
numeric_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
target = 'target'
X = df[numeric_features + categorical_features]
y = df[target]

# 3. Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Preprocessing Pipeline
# As with ensembles, we only need to OneHotEncode. No scaling is needed.
categorical_transformer = OneHotEncoder(handle_unknown='ignore')
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep numeric features as-is
)

# 5. Apply Preprocessing
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# 6. Initialize and Train Model
# --- CRITICAL FOR YOUR EXAM ---
# criterion='entropy' tells the model to use Information Gain.
# max_depth=5 is a 'pre-pruning' step to prevent overfitting.
model = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)
model.fit(X_train_processed, y_train)

# 7. Evaluate Model
y_pred = model.predict(X_test_processed)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Performance (Decision Tree):")
print(f"Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 8. (Optional) Visualize the Tree
# This is the "white-box" benefit!
# We must get the feature names after encoding
feature_names = preprocessor.get_feature_names_out()
plt.figure(figsize=(20,10))
tree.plot_tree(model, feature_names=list(feature_names), class_names=['No Disease', 'Disease'], filled=True)
plt.title("Decision Tree Visualization")
plt.show()
</pre>
</body>
</html>
